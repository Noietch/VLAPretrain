[base]
type = ml-easy-job

[resource]
usergroup = hadoop-aipnlp
queue = root.shxs_training_cluster.hadoop-aipnlp.h800_eval

[roles]
worker.script = /mnt/dolphinfs/ssd_pool/docker/user/hadoop-aipnlp/EVA/yangheqing/envs/lerobot_rdt/bin/python /mnt/dolphinfs/ssd_pool/docker/user/hadoop-nlp-sh02/native_mm/yiyang11/data_pross/1_convert_parquet_to_jsonl/convert_parquet_to_jsonl.py
workers = 1
worker.memory = 30240
worker.vcore = 100
worker.gcoresh20-141g = 0
worker.ports = 1

[am]
afo.app.am.resource.mb = 7168
am.vcore = 4

[docker]
afo.docker.image.name = registryonline-hulk.sankuai.com/custom_prod/com.sankuai.data.hadoop.gpu/data-hadoop-mt-ocr_7zip1-8e03b15a
afo.role.worker.env.INIT_SCRIPT_SSHD_ENABLED = true
afo.role.worker.env.INIT_SCRIPT_SSHD_PASSWORD = abc123
afo.role.worker.env.INIT_SCRIPT_SSHD_PROT = 22

[tensorboard]
with.tensor.board = false

[data]
afo.data.prefetch = false
engine.log.monitor.start_test = false

[conda]

[config]
afo.role.worker.env.INIT_SCRIPT_SSHD_ENABLED = true
afo.role.worker.env.INIT_SCRIPT_SSHD_PASSWORD = abc123
afo.role.worker.env.INIT_SCRIPT_SSHD_PROT = 22

[scheduler]
afo.app.yarn.allocate.timeout.seconds = 0

[others]
afo.app.env.YARN_CONTAINER_RUNTIME_DOCKER_SHM_SIZE_BYTES = 343597383680
afo.app.env.YARN_CONTAINER_RUNTIME_DOCKER_ULIMITS = memlock=-1
afo.app.env.HOPE_IO_HDFS_CONF = {"dfs.default.blocksize":"536870912","dfs.blocksize":"536870912"}
afo.network.mode = RDMA
with_requirements = false
afo.afo-base.image.version = llm_sup
afo.use.acceleration.submission = true
afo.docker.rw.volume.paths = /ssd_pool/docker/user/hadoop-hdp-hldy:/mnt/dolphinfs/ssd_pool/docker/user/hadoop-hdp-hldy,/hdd_pool/docker/user/hadoop-hdp-hldy:/mnt/dolphinfs/hdd_pool/docker/user/hadoop-hdp-hldy
afo.dolphinfs.otherusers = hadoop-aipnlp,hadoop-mlp-ckpt,hadoop-hdp,hadoop-hldy-nlp,hadoop-nlp-sh02
afo.app.env.DFS_CLIENT_WRITE_ZONE = SH02
afo.use.hdfs.fuse = true
afo.use.hdfs.fuse.subpath = :/mnt/hdfs
afo.app.yarn.allocate.timeout.seconds = 0
mlp.notice.phone.call = attempt.delay,job.failed
source.system = kinet
project.name = default

[failover]
afo.app.support.engine.failover = true
afo.role.worker.not.nccl_not_ready = true
afo.role.worker.task.attempt.max.retry = 8
afo.use.nic.down.new.strategy = true
afo.role.worker.env.INIT_SCRIPT_SSHD_ENABLED = true
afo.role.worker.env.INIT_SCRIPT_SSHD_PASSWORD = abc123
afo.role.worker.env.INIT_SCRIPT_SSHD_PROT = 22
afo.role.worker.env.NCCL_DEBUG = INFO
afo.role.worker.env.NCCL_SOCKET_IFNAME = ^lo,docker0
afo.role.worker.env.NCCL_IB_GID_INDEX = 7
afo.role.worker.env.HOPE_PROFILER_ENABLED = false
afo.role.worker.env.HOPE_PROFILER_EXPORT_DIR = /mnt/dolphinfs/ssd_pool/docker/user/hadoop-aipnlp/liangjianze/profiling/
afo.role.worker.env.HOPE_PROFILER_RANK_LIST = 0,1,2,3
afo.role.worker.env.HOPE_PROFILER_SKIP_STEPS = 2
afo.role.worker.env.HOPE_PROFILER_WAIT_STEPS = 2
afo.role.worker.env.HOPE_PROFILER_WARM_STEPS = 2
afo.role.worker.env.HOPE_PROFILER_ACTV_STEPS = 2
afo.role.worker.env.HOPE_PROFILER_REPT_STEPS = 1
afo.use.host_network = false
afo.host_network.min_num = 0
afo.app.env.NCCL_SOCKET_IFNAME = eth5,br0
afo.use.hadoop.client = true
afo.use.hadoop.version = v2
client.env.HOPE_LOGIN_MODE = dx_confirm
hope.base_workdir = 

